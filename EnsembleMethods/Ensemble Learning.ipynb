{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAIM 2018\n",
    "\n",
    "## Ensemble Methods - Bagging and Boosting\n",
    "### Anand Subramanian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "np.random.seed(8088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging(object):\n",
    "    def __init__(self, model, x_data, y_data):\n",
    "        self.x_data = np.array(x_data)\n",
    "        self.y_data = np.array(y_data)\n",
    "        self.N = self.x_data.shape[0]\n",
    "        self.model = model\n",
    "    \n",
    "    def bagging_train(self, Num_learners = 1):\n",
    "        self.learners = []\n",
    "        for k in range(Num_learners):\n",
    "            learner = self.model()\n",
    "            for i in range(10):\n",
    "                # Bootstrap data\n",
    "                idx = np.random.randint(self.N, size = int(self.N/Num_learners))\n",
    "                x_train = self.x_data[idx, :]\n",
    "                y_train = self.y_data[idx]\n",
    "                \n",
    "                # Train Classifiers\n",
    "                train_model = learner.fit(x_train, y_train)\n",
    "            self.learners.append(learner)\n",
    "            \n",
    "    def get_predictions(self,x_test, y_test):\n",
    "        y_pred = np.empty([x_test.shape[0], 0])\n",
    "       \n",
    "        for i, learner in enumerate(self.learners):\n",
    "            y_pred = np.hstack((y_pred, learner.predict(x_test).reshape(-1,1)))\n",
    "            \n",
    "        # Plurality Voting\n",
    "        y_pred += 1\n",
    "        preds = []\n",
    "        for row in y_pred:\n",
    "            preds.append(np.argmax(np.bincount(row.astype(int)))-1)\n",
    "        self.test_accuracy = 100*(y_test == preds).sum()/y_test.shape[0]\n",
    "        return preds, self.test_accuracy      \n",
    "        \n",
    "\"\"\"===================================================================================================\"\"\"\n",
    "class Adaboost(object):\n",
    "    def __init__(self, model, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.N = self.x_data.shape[0]\n",
    "        self.weights = np.ones(self.N)/self.N\n",
    "        self.eps = []\n",
    "        self.alpha = []\n",
    "        self.learners = []\n",
    "        self.model = model\n",
    "        \n",
    "    def boost(self, Num_learners= 50):\n",
    "        self.weights = np.ones(self.N)/self.N\n",
    "        self.eps = []\n",
    "        self.alpha = []\n",
    "        self.learners = []\n",
    "        for k in range(Num_learners): \n",
    "            learner = self.model(max_depth = 1, random_state = 1)\n",
    "            #Train the classifier\n",
    "            train_model = learner.fit(self.x_data, self.y_data, sample_weight=self.weights)\n",
    "            \n",
    "            # Get predictions\n",
    "            y_pred = learner.predict(self.x_data)\n",
    "            \n",
    "            e_k = np.sum((y_pred != self.y_data)*self.weights)\n",
    "            self.eps.append(e_k)\n",
    "            \n",
    "            alpha_k = 0.5*np.log((1 - e_k) / float(e_k))\n",
    "            self.alpha.append(alpha_k)\n",
    "            \n",
    "            # Update the weights\n",
    "            I = np.array([1.0 if x == True else -1.0 for x in (y_pred != self.y_data)])\n",
    "            self.weights = np.multiply(self.weights, np.exp(alpha_k*I))\n",
    "            self.weights = self.weights/ (np.sum(self.weights))\n",
    "            \n",
    "            # added learner to the list\n",
    "            self.learners.append(learner)\n",
    "            \n",
    "    def get_predictions(self, x_test, y_test):\n",
    "        y_pred = np.zeros(x_test.shape[0])\n",
    "        for i, learner in enumerate(self.learners):\n",
    "            #print(learner.predict(x_test).shape, y_pred.shape, self.alpha[i])\n",
    "            y_pred += self.alpha[i]*learner.predict(x_test)\n",
    "        \n",
    "        y_pred = np.sign(y_pred)\n",
    "        # calculate test accuracy\n",
    "        self.test_accuracy = 100*(y_test == y_pred).sum()/y_test.shape[0]\n",
    "        return y_pred, self.test_accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_hastie_10_2()\n",
    "df = pd.DataFrame(x)\n",
    "df['Y'] = y\n",
    "\n",
    "# Split into training and test set\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "X_train, Y_train = train.ix[:,:-1], train.ix[:,-1]\n",
    "X_test, Y_test = test.ix[:,:-1], test.ix[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "ensemble = Bagging(DecisionTreeClassifier,X_train, Y_train)\n",
    "\n",
    "ensemble.bagging_train(Num_learners= 70)  \n",
    "y_pred, test_accuracy = ensemble.get_predictions(X_test, Y_test)\n",
    "print('Test Accuracy of Bagged Decision Trees : %.4f %% '%test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of Decision Stump : 53.7917 % \n",
      "Test Accuracy of Boosted Decision Trees : 90.5000 % \n"
     ]
    }
   ],
   "source": [
    "# Boosting\n",
    "clf_tree = DecisionTreeClassifier(max_depth = 1, random_state = 1)\n",
    "ensemble = Adaboost(DecisionTreeClassifier,X_train, Y_train)\n",
    "\n",
    "# Without Boosting (Single Classifier)\n",
    "ensemble.boost(Num_learners= 1)    \n",
    "y_pred, test_accuracy = ensemble.get_predictions(X_test, Y_test)\n",
    "print('Test Accuracy of Decision Stump : %.4f %% '%test_accuracy)\n",
    "\n",
    "# With Boosting (Miltuple Cl)\n",
    "ensemble.boost(Num_learners= 400)    \n",
    "y_pred, test_accuracy =ensemble.get_predictions(X_test, Y_test)\n",
    "print('Test Accuracy of Boosted Decision Trees : %.4f %% '%test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
